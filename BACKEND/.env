# Porta do backend
PORT=5000

# Ambiente
NODE_ENV=development

# Prefixo das rotas da API
API_PREFIX=/api

# Origem permitida para CORS (ajuste para o endereço do seu frontend)
CORS_ORIGINS=http://localhost:5173,http://localhost:4173,http://localhost:5001,http://127.0.0.1:5001,http://localhost:3000,http://localhost:3001,http://127.0.0.1:3000,http://127.0.0.1:3001

# Limite do body parser
BODY_PARSER_LIMIT=10mb

# Limite de requisições por janela de tempo (rate limit)
RATE_LIMIT_WINDOW_MINUTES=15
RATE_LIMIT_MAX_REQUESTS=100

# Versão do app
APP_VERSION=1.0.0

# URL base da API
API_URL=http://localhost:5000

# URL da API para o frontend (Next.js/React)
NEXT_PUBLIC_API_URL=http://localhost:5000

# URL do frontend
FRONTEND_URL=http://localhost:3000

# Configurações de upload
MAX_UPLOAD_SIZE_MB=50
ALLOWED_UPLOAD_TYPES=.pdf,.jpg,.jpeg,.png
UPLOAD_STORAGE_DIR=uploads/
UPLOAD_TMP_DIR=/tmp/uploads

# Configurações de cache
CACHE_TTL_SECONDS=3600
CACHE_CHECK_PERIOD_SECONDS=600

# Redis Configuration
REDIS_HOST=localhost
REDIS_PORT=6379

# LOG
LOG_LEVEL=info

# =========================
# SUPABASE CONFIGURATION
# =========================
SUPABASE_URL=https://yqlfgazngdymiprsrwvf.supabase.co
SUPABASE_ANON_KEY=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6InlxbGZnYXpuZ2R5bWlwcnNyd3ZmIiwicm9sZSI6ImFub24iLCJpYXQiOjE3NTM1NTY3NzYsImV4cCI6MjA2OTEzMjc3Nn0.O5YelU7b0yqpSHSdttC1NL0BSVURoO-mFbBCZSuotj4
SUPABASE_SERVICE_ROLE_KEY=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6InlxbGZnYXpuZ2R5bWlwcnNyd3ZmIiwicm9sZSI6InNlcnZpY2Vfcm9sZSIsImlhdCI6MTc1MzU1Njc3NiwiZXhwIjoyMDY5MTMyNzc2fQ.-DP0gUiUU7G8CCVMwO9Y-kp5recj5iv_JXKMR72U8lA

# Firebase Hash Parameters for Password Migration
FIREBASE_SIGNER_KEY=ocYNJ7HJL9+4nWkRHlz8mxPtnJ2/7R6mg8UJGDVRYJg7gB5DZay5nJOQ69IYBIXC9Wt6TjvQj8M1WGjZwLOBDA==
FIREBASE_SALT_SEPARATOR=Bw==
FIREBASE_ROUNDS=8
FIREBASE_MEMCOST=14

# =========================
# DATABASE CONFIGURATION
# =========================
DATABASE_URL=postgresql://postgres.yqlfgazngdymiprsrwvf:qh2cX0xoyCzftrzB@aws-0-sa-east-1.pooler.supabase.com:6543/postgres
DB_HOST=aws-0-sa-east-1.pooler.supabase.com
DB_PORT=6543
DB_NAME=postgres
DB_USER=postgres.yqlfgazngdymiprsrwvf
DB_PASSWORD=qh2cX0xoyCzftrzB

# =========================
# PULSE AI - GOOGLE GENERATIVE AI
# =========================
# ⚠️  IMPORTANTE: Substitua pela sua chave real do Google AI Studio
GOOGLE_AI_API_KEY=AIzaSyCSphKKYt8F4FTXCMZnY7Fwiqnt1RvWoT8

# Configurações opcionais do PULSE AI
PULSE_AI_MODEL=gemini-2.5-flash-lite-preview-06-17
PULSE_AI_TEMPERATURE=0.3
PULSE_AI_MAX_TOKENS=65535
PULSE_AI_ENABLE_LOGGING=true
PULSE_AI_LOG_LEVEL=info

# =========================
# OPENROUTER AI - Question Categorization
# =========================
OPENROUTER_API_KEY=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

# =========================
# LM STUDIO - Local AI (100% Gratuito)
# =========================
# Ativar LM Studio ao invés do OpenRouter (true = local, false = OpenRouter)
USE_LM_STUDIO=false
# URL do servidor LM Studio (padrão: http://localhost:1234/v1)
LM_STUDIO_URL=http://localhost:1234/v1
# Nome do modelo carregado no LM Studio (usar o ID exato do modelo)
LM_STUDIO_MODEL=openai/gpt-oss-20b

# =========================
# GOOGLE GEMINI - 100% Gratuito com 1M tokens de contexto
# =========================
# Ativar Google Gemini (true = usar Gemini, false = usar OpenRouter/LM Studio)
USE_GEMINI=false
USE_MINIMAX=true
USE_GPT_OSS=false
# Você já tem essa chave configurada acima (GOOGLE_AI_API_KEY)
# Modelo Gemini a usar (opções: gemini-2.5-flash, gemini-1.5-flash, gemini-1.5-pro, gemini-2.0-flash-exp)
GEMINI_MODEL=gemini-2.5-flash

# =========================
# MINIMAX M2 - Agentic Model with Interleaved Thinking (200k input, 128k output!)
# =========================
# API Key 1 - Usada para CATEGORIZAÇÃO
MINIMAX_API_KEY=eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJHcm91cE5hbWUiOiJJbm9jZW5jaW8gSnVuaW9yIiwiVXNlck5hbWUiOiJJbm9jZW5jaW8gSnVuaW9yIiwiQWNjb3VudCI6IiIsIlN1YmplY3RJRCI6IjE5ODU1MjkzNjYxOTIzMzM1MjciLCJQaG9uZSI6IiIsIkdyb3VwSUQiOiIxOTg1NTI5MzQxMzg3MjE1NTc2IiwiUGFnZU5hbWUiOiIiLCJNYWlsIjoiaW5vY2VuY2lvLjEyM0BnbWFpbC5jb20iLCJDcmVhdGVUaW1lIjoiMjAyNS0xMS0wNCAxMTo0NjozNyIsIlRva2VuVHlwZSI6MSwiaXNzIjoibWluaW1heCJ9.yNw2ZM8grjQvzPki9T-6_SPNJp6WzgW5GDrJZQ1sS3BYYkxx3vkbL1EJmjEUPjHyZNmwPcBcZy7g6s3nkar-usgICSkO2CqZUhHXfqhloKnMf2DG0XuWSDYB0iCxEYeMbaK7eRsSwf514Sgji200TCW4QD2ggpzO8sdBVMNNSpL_x9wenDt_a302oMMBt3hz87yc8pto03mMaeUcu28lyK_lNA4aj6eChvCekddeFVDDT0s_tORhj2XNmkeg3LTmqkXWtYv2hZsf7SRmnzXQOC8KC6PxTuT2-32xZbVD_DImQJvfagPt6pQw5H6OUShU7iH1SIaIKMmWwJE3ZoetKw

# API Key 2 - Usada para REESCRITA DE COMENTÁRIOS (processamento paralelo)
MINIMAX_API_KEY_2=eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJHcm91cE5hbWUiOiJWw61kZW9zIFVQRSBDREUgM3JvIEMiLCJVc2VyTmFtZSI6IlbDrWRlb3MgVVBFIENERSAzcm8gQyIsIkFjY291bnQiOiIiLCJTdWJqZWN0SUQiOiIxOTg2MTg1MDI3NTc0MDQ3NDE1IiwiUGhvbmUiOiIiLCJHcm91cElEIjoiMTk4NjE4NTAyNzU2NTY1NDcxMSIsIlBhZ2VOYW1lIjoiIiwiTWFpbCI6InZpZGVvc3VwZTIwMTcuMUBnbWFpbC5jb20iLCJDcmVhdGVUaW1lIjoiMjAyNS0xMS0wNiAwNjo0NDo0NSIsIlRva2VuVHlwZSI6MSwiaXNzIjoibWluaW1heCJ9.gtCzWZWwz1StdvuhMWyisZIb3ab6MdEuMmaV24leX93qjCIxw66EphOgGMGZSaAvNbjQvdHCwTb-_zJ9fIPgMBrtAsQeWR4UzSsWZlemqgrAP7TJj90NYGosuUBqLS4baSgZask5YPcdkuUa67Vi7sxmpqA7qxYU2mOkg5j-KdCiax92QGeDCvywXz5AxZFSkFqA0ZMXEf6ihxaBW86uBDLzMSsstASc7OesmhzEGlp1B7a6T8OK1IXOfsVfDBNKi149hd9H4aWBJOftUEwxBA0o2n2TYAWBl9h2h70w-ghcbxRatg5_WV3ifqChdrrGlpd3WKmQGXDYYMkZ0DlJbw

MINIMAX_MODEL=MiniMax-M2
MINIMAX_BASE_URL=https://api.minimax.io/anthropic
# Max output tokens (128K available, no limits on API!)
MINIMAX_MAX_TOKENS=128000

# =========================
# GPT-OSS (OpenAI via Hugging Face) - 120B params, high reasoning
# =========================
GPT_OSS_API_KEY=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
GPT_OSS_MODEL=openai/gpt-oss-120b:fireworks-ai
GPT_OSS_BASE_URL=https://router.huggingface.co/v1

# =========================
# QWEN3-235B-A22B (Hugging Face) - 235B params, 22B active, thinking mode
# =========================
USE_QWEN=false
QWEN_API_KEY=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
QWEN_MODEL=Qwen/Qwen3-235B-A22B-Thinking-2507
QWEN_BASE_URL=https://router.huggingface.co/v1
# Enable thinking mode for complex reasoning (true) or non-thinking for efficiency (false)
QWEN_ENABLE_THINKING=true
# Max context length: 32768 (Router API limit, 256K native context available with self-hosting)
QWEN_MAX_CONTEXT=32768

# =========================
# GLM-4.6 (Hugging Face) - 200K context, fast reasoning
# =========================
USE_GLM=false
GLM_API_KEY=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
GLM_MODEL=zai-org/GLM-4.6
GLM_BASE_URL=https://router.huggingface.co/v1
# Max output tokens (200K total context available)
GLM_MAX_CONTEXT=32768

# =========================
# CREDENCIAIS FIREBASE ADMIN (ATUALIZADAS)
# =========================

# =========================
# Azure Document Intelligence Configuration
# =========================
AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT=https://medpulse.cognitiveservices.azure.com/
AZURE_DOCUMENT_INTELLIGENCE_KEY=6kH3yz3xOpF5sRfN6nTaOUM3czDclPW2qIVEXhzVCpjeK5ur3pnSJQQJ99BEACZoyfiXJ3w3AAALACOGnwkh

# =========================
# PDF.CO API Configuration
# =========================
PDF_CO_API_KEY=inocencio.123@gmail.com_kTpTlpi5bh9YFNMKFxlyB5dEu5tjPrI1wed4ZzdKRzq7FMat9TRmPK4hOKEWzdp7
PDF_CO_BASE_URL=https://api.pdf.co/v1

# =========================
# Cloudflare R2 Storage Configuration (BACKEND ONLY - SEGURO)
# =========================
R2_ACCOUNT_ID=16fc5a72ff773d4925e9e5a1b0136737
R2_BUCKET_NAME=medbrave
R2_ACCESS_KEY_ID=41c779389c2f6cd8039d2537cced5a69
R2_SECRET_ACCESS_KEY=f99e3b6cc38730d0a8ccb266a8adedb9a677ed5308a8a39b18edd8b43dbb2a78
R2_ENDPOINT=https://16fc5a72ff773d4925e9e5a1b0136737.r2.cloudflarestorage.com
R2_PUBLIC_URL=https://medbrave.com.br
VITE_R2_PUBLIC_URL=https://medbrave.com.br